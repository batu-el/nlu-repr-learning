{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"G3Y3Y3oDSnBp"},"outputs":[],"source":["### Deep Feature Reweighting ###"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jl9Fhg5HSnBq"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","drive_PATH = '../content/drive/MyDrive/Colab Notebooks/dis.experiments.4'\n","import sys\n","sys.path.append(drive_PATH)\n","# drive_PATH = ''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uenbkl5aSnBr"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","import utils.NLIdataset as nli_ds\n","import utils.transforms as tr\n","\n","import tqdm\n","import math\n","import numpy as np\n","import pandas as pd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sLDWl9uNSnBr"},"outputs":[],"source":["# Device for GPU speedup\n","DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","DEVICE"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sz8W2GDASnBr"},"outputs":[],"source":["### MNLI Dataset ###\n","!pip install jsonlines\n","import jsonlines # jsonl imports\n","\n","train_PATH = drive_PATH + '/data/multinli_1.0/multinli_1.0_train.jsonl'\n","dev_matched_PATH = drive_PATH + '/data/multinli_1.0/multinli_1.0_dev_matched.jsonl'\n","dev_mismatched_PATH = drive_PATH + '/data/multinli_1.0/multinli_1.0_dev_mismatched.jsonl'\n","hans_PATH = drive_PATH + '/data/hans/heuristics_evaluation_set.jsonl'\n","\n","# Train Data\n","train_DATA = []\n","train_s1 = []\n","train_s2 = []\n","train_text = []\n","train_label = []\n","# Mathced Dev Data\n","dev_matched_DATA = []\n","dev_matched_s1 = []\n","dev_matched_s2 = []\n","dev_matched_text = []\n","dev_matched_label = []\n","# Mismatched Dev Data\n","dev_mismatched_DATA = []\n","dev_mismatched_s1 = []\n","dev_mismatched_s2 = []\n","dev_mismatched_text = []\n","dev_mismatched_label = []\n","# Hans Data\n","hans_DATA = []\n","hans_s1 = []\n","hans_s2 = []\n","hans_text = []\n","hans_label = []\n","\n","with jsonlines.open(train_PATH) as f:\n","    for line in f.iter():\n","        train_DATA.append(line)\n","        train_s1.append(line['sentence1'])\n","        train_s2.append(line['sentence2'])\n","        train_text.append( line['sentence1'] + ' ' + line['sentence2'] )\n","        train_label.append(line['gold_label'])\n","with jsonlines.open(dev_matched_PATH) as f:\n","    for line in f.iter():\n","        dev_matched_DATA.append(line)\n","        dev_matched_s1.append(line['sentence1'])\n","        dev_matched_s2.append(line['sentence2'])\n","        dev_matched_text.append( line['sentence1'] + ' ' + line['sentence2'] )\n","        dev_matched_label.append(line['gold_label'])\n","with jsonlines.open(dev_mismatched_PATH) as f:\n","    for line in f.iter():\n","        dev_mismatched_DATA.append(line)\n","        dev_mismatched_s1.append(line['sentence1'])\n","        dev_mismatched_s2.append(line['sentence2'])\n","        dev_mismatched_text.append( line['sentence1'] + ' ' + line['sentence2'] )\n","        dev_mismatched_label.append(line['gold_label'])\n","with jsonlines.open(hans_PATH) as f:\n","    for line in f.iter():\n","        hans_DATA.append(line)\n","        hans_s1.append(line['sentence1'])\n","        hans_s2.append(line['sentence2'])\n","        hans_text.append( line['sentence1'] + ' ' + line['sentence2'] )\n","        hans_label.append(line['gold_label'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AMxUCL2fSnBr"},"outputs":[],"source":["### Cleaning Datasets\n","\n","# Train\n","train_label = np.array(train_label, dtype='<U14')\n","train_s1 = np.array(train_s1)\n","train_s2 = np.array(train_s2)\n","train_label[(train_label == 'neutral') | (train_label == 'contradiction')] = 'non-entailment'\n","train_label[train_label == ['entailment']] = 1\n","train_label[train_label == ['non-entailment']] = 0\n","train_label = np.array(train_label, dtype='int')\n","\n","# Dev Matched\n","dev_matched_label = np.array(dev_matched_label, dtype='<U14')\n","dev_matched_filter = dev_matched_label != '-'\n","dev_matched_s1 = np.array(dev_matched_s1)[dev_matched_filter]\n","dev_matched_s2 = np.array(dev_matched_s2)[dev_matched_filter]\n","dev_matched_label = dev_matched_label[dev_matched_filter]\n","dev_matched_label[(dev_matched_label == 'neutral') | (dev_matched_label == 'contradiction')] = 'non-entailment'\n","dev_matched_label[dev_matched_label == ['entailment']] = 1\n","dev_matched_label[dev_matched_label == ['non-entailment']] = 0\n","dev_matched_label = np.array(dev_matched_label, dtype='int')\n","\n","# Dev Mismatched\n","dev_mismatched_label = np.array(dev_mismatched_label, dtype='<U14')\n","dev_mismatched_filter = dev_mismatched_label != '-'\n","dev_mismatched_s1 = np.array(dev_mismatched_s1)[dev_mismatched_filter]\n","dev_mismatched_s2 = np.array(dev_mismatched_s2)[dev_mismatched_filter]\n","dev_mismatched_label = dev_mismatched_label[dev_mismatched_filter]\n","dev_mismatched_label[(dev_mismatched_label == 'neutral') | (dev_mismatched_label == 'contradiction')] = 'non-entailment'\n","dev_mismatched_label[dev_mismatched_label == ['entailment']] = 1\n","dev_mismatched_label[dev_mismatched_label == ['non-entailment']] = 0\n","dev_mismatched_label = np.array(dev_mismatched_label, dtype='int')\n","\n","# HANS\n","hans_label = np.array(hans_label)\n","hans_s1 = np.array(hans_s1)\n","hans_s2 = np.array(hans_s2)\n","hans_label[hans_label == ['entailment']] = 1\n","hans_label[hans_label == ['non-entailment']] = 0\n","hans_label = np.array(hans_label, dtype='int')\n","\n","train_labels = np.unique(train_label)\n","dev_matched_labels = np.unique(dev_matched_label)\n","dev_mismatched_labels = np.unique(dev_mismatched_label)\n","hans_labels = np.unique(np.array(hans_label))\n","\n","value_counts = pd.concat({'train_label' : pd.DataFrame(train_label).value_counts(),\n","                        'dev_matched_label' : pd.DataFrame(dev_matched_label).value_counts(),\n","                        'dev_mismatched_label' : pd.DataFrame(dev_mismatched_label).value_counts(),\n","                        'hans_label' : pd.DataFrame(hans_label).value_counts()})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wLe1arSPSnBs"},"outputs":[],"source":["### Balancing Act\n","def balanced_idx(label_dataset):\n","    idx1 = np.array(range(len(label_dataset)))[label_dataset == 1]\n","    idx0 = np.array(range(len(label_dataset)))[label_dataset == 0]\n","    idx0_selected_i = np.random.choice(idx0.shape[0], len(idx1), replace=False)\n","    idx0_selected = idx0[idx0_selected_i]\n","    idx = np.concatenate((idx1, idx0_selected))\n","    np.random.shuffle(idx) # random shuffle\n","    return idx\n","\n","# Balancing Train\n","train_balanced_idx = balanced_idx(train_label)\n","train_s1 = train_s1[train_balanced_idx]\n","train_s2 = train_s2[train_balanced_idx]\n","train_label = train_label[train_balanced_idx]\n","\n","# Balancing Dev Matched\n","dev_matched_balanced_idx = balanced_idx(dev_matched_label)\n","dev_matched_s1 = dev_matched_s1[dev_matched_balanced_idx]\n","dev_matched_s2 = dev_matched_s2[dev_matched_balanced_idx]\n","dev_matched_label = dev_matched_label[dev_matched_balanced_idx]\n","\n","# Balancing Dev Mismatched\n","dev_mismatched_balanced_idx = balanced_idx(dev_mismatched_label)\n","dev_mismatched_s1 = dev_mismatched_s1[dev_mismatched_balanced_idx]\n","dev_mismatched_s2 = dev_mismatched_s2[dev_mismatched_balanced_idx]\n","dev_mismatched_label = dev_mismatched_label[dev_mismatched_balanced_idx]\n","\n","# Balancing HANS (already balanced)\n","hans_balanced_idx = balanced_idx(hans_label)\n","hans_s1 = hans_s1[hans_balanced_idx]\n","hans_s2 = hans_s2[hans_balanced_idx]\n","hans_label = hans_label[hans_balanced_idx]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-mimONIJSnBs"},"outputs":[],"source":["### Preprocessing ###\n","vocab_train_iter = nli_ds.NLIdataset_merge(train_text , np.array(train_label, dtype='str'))\n","token_transform = tr.construct_token_transform()\n","vocab_transform = tr.construct_vocab_transform(vocab_train_iter)\n","tensor_transform = tr.construct_tensor_transform()\n","text_transform = tr.construct_text_transform(token_transform , vocab_transform, tensor_transform)\n","VOCAB_SIZE = len(vocab_transform)\n","VOCAB_SIZE"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"brA7ZGYWSnBs"},"outputs":[],"source":["from model.embedding import TokenEmbedding, PositionalEncoding\n","from model.classifier import NonLinearClassifier\n","from model.encoder import Transformer_Encoder\n","\n","\n","### Natural Language Inference Model\n","class NLInference(nn.Module):\n","    def __init__(self):\n","        super(NLInference, self).__init__()\n","        # Configuration and Initialization\n","        self.dmodel = 256                       # All\n","        self.num_enc_layers = 2                 # Encoder\n","        self.nhead = 4                          # Encoder: For Transformer\n","        self.dclassifier = 2*self.dmodel        # Classifier: Calculate the input dimension for the classifier\n","        self.fc_dim = 512                       # Classifier: Dimension of the fully connected layers\n","        self.n_classes = 2                      # Classifier: Number of classes for classification\n","\n","        # Encoders\n","        self.encoder = Transformer_Encoder( self.dmodel , self.nhead, self.num_enc_layers, VOCAB_SIZE )\n","        # Classifiers\n","        self.classifier = NonLinearClassifier(self.dclassifier, self.fc_dim, self.n_classes)\n","\n","    def forward(self, s1, s2):\n","        # padding masks\n","        # s1_padding_mask = (s1 == tr.PAD_IDX).transpose(0, 1)\n","        # s2_padding_mask = (s2 == tr.PAD_IDX).transpose(0, 1)\n","        # add masks s1_padding_mask, s2_padding_mask\n","        # s1_emb = self.positional_encoding(self.tok_emb(s1))\n","        # s2_emb = self.positional_encoding(self.tok_emb(s2))\n","        # pass embeddings through encoder\n","        s1_encoded = self.encoder(s1)\n","        s2_encoded = self.encoder(s2)\n","        # take the average to calculate sentence representation\n","        # s1_encoded = torch.sum(s1_encoded,0) / s1_encoded.size()[0]\n","        # s2_encoded = torch.sum(s2_encoded,0) / s2_encoded.size()[0]\n","        # combine the two sentences by concatenating\n","        combined_context = torch.cat((s1_encoded, s2_encoded), 1)\n","        # Pass the combined features through the classifier to get the output\n","        output = self.classifier(combined_context)\n","        return output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2OTpmuzBSnBs"},"outputs":[],"source":["model = NLInference()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hCJfhiPkSnBs"},"outputs":[],"source":["### TRAINING LOOP\n","import time\n","def train(dataloader):\n","    # print('HERE')\n","    model.cuda()\n","    model.train()\n","    total_acc, total_count = 0, 0\n","    log_interval = 100\n","    start_time = time.time()\n","\n","    for idx, (s1, s2, label) in enumerate(dataloader):\n","        s1 = s1.to(DEVICE)\n","        s2 = s2.to(DEVICE)\n","        label = label.to(DEVICE)\n","\n","        optimizer.zero_grad()\n","        predicted_label = model(s1, s2)\n","\n","        loss = criterion(predicted_label, label)\n","        loss.backward()\n","\n","        # torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1) # investigate\n","\n","        optimizer.step()\n","\n","        total_acc += (predicted_label.argmax(1) == label).sum().item()\n","        total_count += label.size(0)\n","\n","        train_acc = total_acc / total_count\n","\n","        if idx % log_interval == 0 and idx > 0:\n","            elapsed = time.time() - start_time\n","            print(\n","                \"| epoch {: d} | {:5d}/{:5d} batches \"\n","                \"| accuracy {:8.3f}\".format(epoch, idx, len(dataloader), train_acc)\n","            )\n","            total_acc, total_count = 0, 0\n","            start_time = time.time()\n","\n","        train_losses.append(train_acc)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WGC3AUx5SnBs"},"outputs":[],"source":["### EVALUATION LOOP\n","def evaluate(dataloader):\n","    model.eval()\n","    total_acc, total_count = 0, 0\n","\n","    with torch.no_grad():\n","        for idx, (s1, s2, label) in enumerate(dataloader):\n","            s1 = s1.to(DEVICE)\n","            s2 = s2.to(DEVICE)\n","            label = label.to(DEVICE)\n","            predicted_label = model(s1, s2)\n","            loss = criterion(predicted_label, label)\n","            total_acc += (predicted_label.argmax(1) == label).sum().item()\n","            total_count += label.size(0)\n","    return total_acc / total_count"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jrVnzfpzSnBs"},"outputs":[],"source":["from torch.utils.data.dataset import random_split\n","from torchtext.data.functional import to_map_style_dataset\n","from torch.utils.data import DataLoader\n","\n","# Hyperparameters\n","EPOCHS = 4  # epoch\n","LEARNING_RATE = 0.0001  # learning rate\n","BATCH_SIZE = 16  # batch size for training\n","\n","criterion = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.98), eps=1e-9)\n","# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n","\n","train_iter = nli_ds.NLIdataset(train_s1 , train_s2, train_label)\n","dev_matched_iter = nli_ds.NLIdataset(dev_matched_s1, dev_matched_s2 , dev_matched_label)\n","dev_mismatched_iter = nli_ds.NLIdataset(dev_mismatched_s1, dev_mismatched_s2 , dev_mismatched_label)\n","hans_iter = nli_ds.NLIdataset(hans_s1, hans_s2 , hans_label)\n","\n","train_dataset = to_map_style_dataset(train_iter)\n","dev_matched_dataset = to_map_style_dataset(dev_matched_iter)\n","dev_mismatched_dataset = to_map_style_dataset(dev_mismatched_iter)\n","hans_dataset = to_map_style_dataset(hans_iter)\n","\n","num_train = int(len(train_dataset) * 0.95)\n","split_train_, split_valid_ = random_split( train_dataset, [num_train, len(train_dataset) - num_train] )\n","num_train_hans = int(len(hans_dataset) * 0.75)\n","split_train_hans_ , split_test_hans_ = random_split( hans_dataset, [num_train_hans, len(hans_dataset) - num_train_hans] )\n","\n","num_train_hans_train_ = int(len(split_train_hans_) * 0.90)\n","num_train_hans_train_ , num_train_hans_valid_ = random_split( split_train_hans_, [num_train_hans_train_, len(split_train_hans_) - num_train_hans_train_] )\n","\n","def collate_fn( batch):\n","    label_pipeline = lambda x: int(x) #{'contradiction': 0, 'entailment': 1, 'neutral': 2, '-': -1}[x]\n","    # lists to hold processed source and target\n","    s1_batch, s2_batch, tgt_batch, padding_offsets = [], [], [],  []\n","    for s1_sample, s2_sample, tgt_sample in batch:\n","        # convert to tensor\n","        s1_sample = text_transform(s1_sample)\n","        s2_sample = text_transform(s2_sample)\n","        s1_batch.append(s1_sample)\n","        s2_batch.append(s2_sample)\n","        tgt_batch.append(label_pipeline(tgt_sample))\n","        padding_offsets.append(len(s1_sample))\n","        padding_offsets.append(len(s2_sample))\n","    # Convert the label_list to a tensor with integer type.\n","    tgt_batch = torch.tensor(tgt_batch, dtype=torch.int64)\n","    # to make the padded sequences for s1 and s2 equal length\n","    padding_offset = max(padding_offsets)\n","    s1_batch[0] = nn.ConstantPad1d((0,padding_offset - len(s1_batch[0]) ), tr.PAD_IDX)(s1_batch[0])\n","    s2_batch[0] = nn.ConstantPad1d((0,padding_offset - len(s2_batch[0]) ), tr.PAD_IDX)(s2_batch[0])\n","    # pad the sequences to ensure they have the same length\n","    s1_batch = torch.nn.utils.rnn.pad_sequence(s1_batch, padding_value=tr.PAD_IDX)\n","    s2_batch = torch.nn.utils.rnn.pad_sequence(s2_batch, padding_value=tr.PAD_IDX)\n","    return s1_batch.to(DEVICE), s2_batch.to(DEVICE), tgt_batch.to(DEVICE)\n","\n","train_dataloader = DataLoader( split_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn )\n","valid_dataloader = DataLoader( split_valid_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn )\n","dev_matched_dataloader = DataLoader( dev_matched_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn )\n","dev_mismatched_dataloader = DataLoader( dev_mismatched_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn )\n","\n","test_hans_dataloader = DataLoader( split_test_hans_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn )\n","train_hans_dataloader = DataLoader( num_train_hans_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn )\n","valid_hans_dataloader = DataLoader( num_train_hans_valid_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn )\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VK9wI2ybSnBt"},"outputs":[],"source":["### INITIALIZATION\n","for p in model.parameters():\n","    if p.dim() > 1:\n","        torch.nn.init.xavier_uniform_(p)\n","\n","### ACCOUNTING\n","model_paths = []\n","val_losses = []\n","train_losses = []\n","import time\n","model_id = '-'.join(time.ctime(time.time()).replace(':', ' ').split(' ')[2:5])\n","\n","### TRAINING\n","for epoch in range(1, EPOCHS + 1):\n","\n","    epoch_start_time = time.time()\n","    train(train_dataloader)\n","    epoch_end_time = time.time()\n","    elapsed_time = epoch_end_time - epoch_start_time\n","\n","    accu_train = train_losses[-1]\n","    accu_val = evaluate(valid_dataloader)\n","    val_losses.append(accu_val)\n","\n","    accu_dev_matched = evaluate(dev_matched_dataloader)\n","    accu_dev_mismatched = evaluate(dev_mismatched_dataloader)\n","    accu_hans = evaluate(test_hans_dataloader)\n","\n","    # register model path\n","    model_paths.append(f'id-{model_id}-epoch-{epoch}-accu_train-{accu_train:.3f}-accu_val-{accu_val:.3f}-accu_dev_matched-{accu_dev_matched:.3f}-accu_dev_mismatched-{accu_dev_mismatched:.3f}-accu_hans-{accu_hans:.3f}.pt')\n","    # save model to path\n","    torch.save(model.state_dict(), drive_PATH+'/model_states/'+model_paths[-1])\n","\n","\n","    print(\"-\" * 59)\n","    print(\"| end of epoch {:3d} | time: {:5.2f}s | valid accuracy {:8.3f} |\".format( epoch, elapsed_time, accu_val))\n","    print(\"| dev matched accuracy {:8.3f} | dev mismatched accuracy {:8.3f} | hans accuracy {:8.3f} |\".format( accu_dev_matched, accu_dev_mismatched, accu_hans))\n","    print(\"-\" * 59)"]},{"cell_type":"code","source":["# Load the model with best validation accuracy\n","best_model_index = np.argmin(val_losses)\n","model_state = model_paths[best_model_index]\n","model.load_state_dict(torch.load(drive_PATH+'/model_states/'+model_state))\n","val_losses"],"metadata":{"id":"T8VoTJzQxJ3v"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wWfq8o7rSnBt"},"outputs":[],"source":["### FREEZING ALL LAYERS EXCEPT THE LAST\n","n_classes = 2\n","for p in model.parameters():\n","    if p.size()[0] == n_classes:\n","        print(p.size())\n","        print(p)\n","        pass\n","    else:\n","        p.requires_grad = False"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"INLq5GyWSnBt"},"outputs":[],"source":["### ACCOUNTING\n","# reinitialize losses and paths\n","model_paths = []\n","val_losses = []\n","train_losses = []\n","\n","EPOCHS = 4\n","\n","### TRAINING\n","for epoch in range(1, EPOCHS + 1):\n","\n","    epoch_start_time = time.time()\n","    train(train_hans_dataloader)\n","    epoch_end_time = time.time()\n","    elapsed_time = epoch_end_time - epoch_start_time\n","\n","    accu_train = train_losses[-1]\n","    accu_val = evaluate(valid_hans_dataloader)\n","    val_losses.append(accu_val)\n","\n","    accu_dev_matched = evaluate(dev_matched_dataloader)\n","    accu_dev_mismatched = evaluate(dev_mismatched_dataloader)\n","    accu_hans = evaluate(test_hans_dataloader)\n","\n","    # register model path\n","    model_paths.append(f'id-{model_id}-DFRepoch-{epoch}-accu_train-{accu_train:.3f}-accu_val-{accu_val:.3f}-accu_dev_matched-{accu_dev_matched:.3f}-accu_dev_mismatched-{accu_dev_mismatched:.3f}-accu_hans-{accu_hans:.3f}.pt')\n","    # save model to path\n","    torch.save(model.state_dict(), drive_PATH+'/model_states/'+model_paths[-1])\n","\n","    print(\"-\" * 59)\n","    print(\"| end of epoch {:3d} | time: {:5.2f}s | valid accuracy {:8.3f} |\".format( epoch, elapsed_time, accu_val))\n","    print(\"| dev matched accuracy {:8.3f} | dev mismatched accuracy {:8.3f} | hans accuracy {:8.3f} |\".format( accu_dev_matched, accu_dev_mismatched, accu_hans))\n","    print(\"-\" * 59)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7a85OZ4ISnBt"},"outputs":[],"source":["import collections\n","from functools import partial\n","\n","NUM_BATCHES = 100\n","s1_dataset, s2_dataset, labels_dataset = [] , [], []\n","for idx, (s1, s2, label) in enumerate(valid_dataloader):\n","\tif idx == NUM_BATCHES:\n","\t\tbreak\n","\ts1_dataset.append(s1)\n","\ts2_dataset.append(s2)\n","\tlabels_dataset.append(label)\n","\n","# a dictionary that keeps saving the activations as they come\n","activations = collections.defaultdict(list)\n","def save_activation(name, mod, inp, out):\n","\tactivations[name].append(out.cpu())\n","\n","# Registering hooks for all the TransformerEncoder layers\n","# Note: Hooks are called EVERY TIME the module performs a forward pass. For modules that are\n","# called repeatedly at different stages of the forward pass (like TransformerEncoder in NLI called for s1 and s2 separately)\n","# this will save different activations.\n","# Editing the forward pass code to save activations is the way to go for these cases.\n","# Or we can filter out the odd and even indices from the activations to get the ones for s1 and s2\n","for name, m in model.named_modules():\n","\tif name == 'encoder':\n","\t\t# partial to assign the layer name to each hook\n","\t\tm.register_forward_hook(partial(save_activation, name))\n","\tif name == 'classifier.classifier.1':\n","\t\tm.register_forward_hook(partial(save_activation, name))\n","\tif name == 'classifier.classifier.4':\n","\t\tm.register_forward_hook(partial(save_activation, name))\n","\tif name == 'classifier.classifier.7':\n","\t\tm.register_forward_hook(partial(save_activation, name))\n","\n","# forward pass through the full dataset\n","for batch_i in range(NUM_BATCHES):\n","\tout = model(s1_dataset[batch_i], s2_dataset[batch_i])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Ravx8ZCSnBt"},"outputs":[],"source":["keys = list(activations.keys())\n","activations_dict = {}\n","activations_df_dict = {}\n","for key in keys:\n","    activations_dict[key] = np.array(torch.cat([a.detach() for a in activations[key]])).transpose(1,0)\n","    activations_df_dict[key] = pd.DataFrame(activations_dict[key])\n","    print(activations_dict[key].shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8nQGhMNtSnBt"},"outputs":[],"source":["activations_df = pd.concat(activations_df_dict, axis=0)\n","out_csv_PATH = drive_PATH + \"/res/activations/DFR/test.csv\"\n","activations_df.to_csv(out_csv_PATH)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}