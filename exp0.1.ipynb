{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"D6dsHVDfSmkG"},"outputs":[],"source":["### Product of Experts ###"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EyH9dcnySmkG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1703290589553,"user_tz":0,"elapsed":24292,"user":{"displayName":"Batu El","userId":"11666366648103508022"}},"outputId":"791b2a02-353d-4d59-b7d0-34fe49825390"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","drive_PATH = '../content/drive/MyDrive/Colab Notebooks/dis.experiments.4'\n","import sys\n","sys.path.append(drive_PATH)\n","# drive_PATH = ''"]},{"cell_type":"code","source":["sys.path"],"metadata":{"id":"ZXSGDYwzTCFn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1703290589553,"user_tz":0,"elapsed":2,"user":{"displayName":"Batu El","userId":"11666366648103508022"}},"outputId":"716e7a24-0df2-4347-e37b-14f273625a20"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['/content',\n"," '/env/python',\n"," '/usr/lib/python310.zip',\n"," '/usr/lib/python3.10',\n"," '/usr/lib/python3.10/lib-dynload',\n"," '',\n"," '/usr/local/lib/python3.10/dist-packages',\n"," '/usr/lib/python3/dist-packages',\n"," '/usr/local/lib/python3.10/dist-packages/IPython/extensions',\n"," '/root/.ipython',\n"," '../content/drive/MyDrive/Colab Notebooks/dis.experiments.4']"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yaqf7sLJSmkG"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","import utils.NLIdataset as nli_ds\n","import utils.transforms as tr\n","\n","import tqdm\n","import math\n","import numpy as np\n","import pandas as pd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K_zpJ4xFSmkH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1703290597482,"user_tz":0,"elapsed":6,"user":{"displayName":"Batu El","userId":"11666366648103508022"}},"outputId":"4acaf1ed-78c6-44aa-b520-e148533b24e2"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{},"execution_count":5}],"source":["# Device for GPU speedup\n","DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","DEVICE"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c3jBvmckSmkH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1703290616731,"user_tz":0,"elapsed":19253,"user":{"displayName":"Batu El","userId":"11666366648103508022"}},"outputId":"7194b6fb-fb62-43fd-85e6-2863d06068f7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting jsonlines\n","  Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n","Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines) (23.1.0)\n","Installing collected packages: jsonlines\n","Successfully installed jsonlines-4.0.0\n"]}],"source":["### MNLI Dataset ###\n","!pip install jsonlines\n","import jsonlines # jsonl imports\n","\n","train_PATH = drive_PATH + '/data/multinli_1.0/multinli_1.0_train.jsonl'\n","dev_matched_PATH = drive_PATH + '/data/multinli_1.0/multinli_1.0_dev_matched.jsonl'\n","dev_mismatched_PATH = drive_PATH + '/data/multinli_1.0/multinli_1.0_dev_mismatched.jsonl'\n","hans_PATH = drive_PATH + '/data/hans/heuristics_evaluation_set.jsonl'\n","\n","# Train Data\n","train_DATA = []\n","train_s1 = []\n","train_s2 = []\n","train_text = []\n","train_label = []\n","# Mathced Dev Data\n","dev_matched_DATA = []\n","dev_matched_s1 = []\n","dev_matched_s2 = []\n","dev_matched_text = []\n","dev_matched_label = []\n","# Mismatched Dev Data\n","dev_mismatched_DATA = []\n","dev_mismatched_s1 = []\n","dev_mismatched_s2 = []\n","dev_mismatched_text = []\n","dev_mismatched_label = []\n","# Hans Data\n","hans_DATA = []\n","hans_s1 = []\n","hans_s2 = []\n","hans_text = []\n","hans_label = []\n","\n","with jsonlines.open(train_PATH) as f:\n","    for line in f.iter():\n","        train_DATA.append(line)\n","        train_s1.append(line['sentence1'])\n","        train_s2.append(line['sentence2'])\n","        train_text.append( line['sentence1'] + ' ' + line['sentence2'] )\n","        train_label.append(line['gold_label'])\n","with jsonlines.open(dev_matched_PATH) as f:\n","    for line in f.iter():\n","        dev_matched_DATA.append(line)\n","        dev_matched_s1.append(line['sentence1'])\n","        dev_matched_s2.append(line['sentence2'])\n","        dev_matched_text.append( line['sentence1'] + ' ' + line['sentence2'] )\n","        dev_matched_label.append(line['gold_label'])\n","with jsonlines.open(dev_mismatched_PATH) as f:\n","    for line in f.iter():\n","        dev_mismatched_DATA.append(line)\n","        dev_mismatched_s1.append(line['sentence1'])\n","        dev_mismatched_s2.append(line['sentence2'])\n","        dev_mismatched_text.append( line['sentence1'] + ' ' + line['sentence2'] )\n","        dev_mismatched_label.append(line['gold_label'])\n","with jsonlines.open(hans_PATH) as f:\n","    for line in f.iter():\n","        hans_DATA.append(line)\n","        hans_s1.append(line['sentence1'])\n","        hans_s2.append(line['sentence2'])\n","        hans_text.append( line['sentence1'] + ' ' + line['sentence2'] )\n","        hans_label.append(line['gold_label'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hrVaun2cSmkH"},"outputs":[],"source":["### Cleaning Datasets\n","\n","# Train\n","train_label = np.array(train_label, dtype='<U14')\n","train_s1 = np.array(train_s1)\n","train_s2 = np.array(train_s2)\n","train_label[(train_label == 'neutral') | (train_label == 'contradiction')] = 'non-entailment'\n","train_label[train_label == ['entailment']] = 1\n","train_label[train_label == ['non-entailment']] = 0\n","train_label = np.array(train_label, dtype='int')\n","\n","# Dev Matched\n","dev_matched_label = np.array(dev_matched_label, dtype='<U14')\n","dev_matched_filter = dev_matched_label != '-'\n","dev_matched_s1 = np.array(dev_matched_s1)[dev_matched_filter]\n","dev_matched_s2 = np.array(dev_matched_s2)[dev_matched_filter]\n","dev_matched_label = dev_matched_label[dev_matched_filter]\n","dev_matched_label[(dev_matched_label == 'neutral') | (dev_matched_label == 'contradiction')] = 'non-entailment'\n","dev_matched_label[dev_matched_label == ['entailment']] = 1\n","dev_matched_label[dev_matched_label == ['non-entailment']] = 0\n","dev_matched_label = np.array(dev_matched_label, dtype='int')\n","\n","# Dev Mismatched\n","dev_mismatched_label = np.array(dev_mismatched_label, dtype='<U14')\n","dev_mismatched_filter = dev_mismatched_label != '-'\n","dev_mismatched_s1 = np.array(dev_mismatched_s1)[dev_mismatched_filter]\n","dev_mismatched_s2 = np.array(dev_mismatched_s2)[dev_mismatched_filter]\n","dev_mismatched_label = dev_mismatched_label[dev_mismatched_filter]\n","dev_mismatched_label[(dev_mismatched_label == 'neutral') | (dev_mismatched_label == 'contradiction')] = 'non-entailment'\n","dev_mismatched_label[dev_mismatched_label == ['entailment']] = 1\n","dev_mismatched_label[dev_mismatched_label == ['non-entailment']] = 0\n","dev_mismatched_label = np.array(dev_mismatched_label, dtype='int')\n","\n","# HANS\n","hans_label = np.array(hans_label)\n","hans_s1 = np.array(hans_s1)\n","hans_s2 = np.array(hans_s2)\n","hans_label[hans_label == ['entailment']] = 1\n","hans_label[hans_label == ['non-entailment']] = 0\n","hans_label = np.array(hans_label, dtype='int')\n","\n","train_labels = np.unique(train_label)\n","dev_matched_labels = np.unique(dev_matched_label)\n","dev_mismatched_labels = np.unique(dev_mismatched_label)\n","hans_labels = np.unique(np.array(hans_label))\n","\n","value_counts = pd.concat({'train_label' : pd.DataFrame(train_label).value_counts(),\n","                        'dev_matched_label' : pd.DataFrame(dev_matched_label).value_counts(),\n","                        'dev_mismatched_label' : pd.DataFrame(dev_mismatched_label).value_counts(),\n","                        'hans_label' : pd.DataFrame(hans_label).value_counts()})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"etj7Llg8SmkH"},"outputs":[],"source":["### Balancing Act\n","def balanced_idx(label_dataset):\n","    idx1 = np.array(range(len(label_dataset)))[label_dataset == 1]\n","    idx0 = np.array(range(len(label_dataset)))[label_dataset == 0]\n","    idx0_selected_i = np.random.choice(idx0.shape[0], len(idx1), replace=False)\n","    idx0_selected = idx0[idx0_selected_i]\n","    idx = np.concatenate((idx1, idx0_selected))\n","    np.random.shuffle(idx) # random shuffle\n","    return idx\n","\n","# Balancing Train\n","train_balanced_idx = balanced_idx(train_label)\n","train_s1 = train_s1[train_balanced_idx]\n","train_s2 = train_s2[train_balanced_idx]\n","train_label = train_label[train_balanced_idx]\n","\n","# Balancing Dev Matched\n","dev_matched_balanced_idx = balanced_idx(dev_matched_label)\n","dev_matched_s1 = dev_matched_s1[dev_matched_balanced_idx]\n","dev_matched_s2 = dev_matched_s2[dev_matched_balanced_idx]\n","dev_matched_label = dev_matched_label[dev_matched_balanced_idx]\n","\n","# Balancing Dev Mismatched\n","dev_mismatched_balanced_idx = balanced_idx(dev_mismatched_label)\n","dev_mismatched_s1 = dev_mismatched_s1[dev_mismatched_balanced_idx]\n","dev_mismatched_s2 = dev_mismatched_s2[dev_mismatched_balanced_idx]\n","dev_mismatched_label = dev_mismatched_label[dev_mismatched_balanced_idx]\n","\n","# Balancing HANS (already balanced)\n","hans_balanced_idx = balanced_idx(hans_label)\n","hans_s1 = hans_s1[hans_balanced_idx]\n","hans_s2 = hans_s2[hans_balanced_idx]\n","hans_label = hans_label[hans_balanced_idx]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y1njPro-SmkH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1703290628908,"user_tz":0,"elapsed":6755,"user":{"displayName":"Batu El","userId":"11666366648103508022"}},"outputId":"97bc4e76-517e-46b7-bd0e-a0d1c539c0fd"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["82054"]},"metadata":{},"execution_count":9}],"source":["### Preprocessing ###\n","vocab_train_iter = nli_ds.NLIdataset_merge(train_text , np.array(train_label, dtype='str'))\n","token_transform = tr.construct_token_transform()\n","vocab_transform = tr.construct_vocab_transform(vocab_train_iter)\n","tensor_transform = tr.construct_tensor_transform()\n","text_transform = tr.construct_text_transform(token_transform , vocab_transform, tensor_transform)\n","VOCAB_SIZE = len(vocab_transform)\n","VOCAB_SIZE"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HQOJQ1adSmkH"},"outputs":[],"source":["from model.embedding import TokenEmbedding, PositionalEncoding\n","from model.classifier import NonLinearClassifier\n","from model.encoder import Transformer_Encoder\n","\n","### Natural Language Inference Model\n","class NLInference(nn.Module):\n","    def __init__(self, dmodel, n_classes):\n","        super(NLInference, self).__init__()\n","        # Configuration and Initialization\n","        self.dmodel = dmodel                    # All\n","        self.num_enc_layers = 2                 # Encoder\n","        self.nhead = 4                          # Encoder: For Transformer\n","        self.dclassifier = 2*self.dmodel        # Classifier: Calculate the input dimension for the classifier\n","        self.fc_dim = 512                       # Classifier: Dimension of the fully connected layers\n","        self.n_classes = n_classes              # Classifier: Number of classes for classification\n","\n","        # Encoders\n","        self.encoder = Transformer_Encoder( self.dmodel , self.nhead, self.num_enc_layers, VOCAB_SIZE )\n","        # Classifiers\n","        self.classifier = NonLinearClassifier(self.dclassifier, self.fc_dim, self.n_classes)\n","\n","    def forward(self, s1, s2):\n","        # padding masks\n","        # s1_padding_mask = (s1 == tr.PAD_IDX).transpose(0, 1)\n","        # s2_padding_mask = (s2 == tr.PAD_IDX).transpose(0, 1)\n","        # add masks s1_padding_mask, s2_padding_mask\n","        # s1_emb = self.positional_encoding(self.tok_emb(s1))\n","        # s2_emb = self.positional_encoding(self.tok_emb(s2))\n","        # pass embeddings through encoder\n","        s1_encoded = self.encoder(s1)\n","        s2_encoded = self.encoder(s2)\n","        # take the average to calculate sentence representation\n","        # s1_encoded = torch.sum(s1_encoded,0) / s1_encoded.size()[0]\n","        # s2_encoded = torch.sum(s2_encoded,0) / s2_encoded.size()[0]\n","        # combine the two sentences by concatenating\n","        combined_context = torch.cat((s1_encoded, s2_encoded), 1)\n","        # Pass the combined features through the classifier to get the output\n","        output = self.classifier(combined_context)\n","        return output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G8SpBLOKSmkH"},"outputs":[],"source":["import torch.nn.functional as F\n","\n","class POELoss(nn.Module):\n","    \"\"\"Implements the product of expert loss.\"\"\"\n","    def __init__(self, size_average=True, ensemble_training=False, poe_alpha=1):\n","        super().__init__()\n","        self.size_average = size_average\n","        self.ensemble_training=ensemble_training\n","        self.poe_alpha = poe_alpha\n","\n","    def compute_probs(self, inputs):\n","        prob_dist = F.softmax(inputs, dim=1)\n","        return prob_dist\n","\n","    def forward(self, inputs, targets, inputs_adv, second_inputs_adv=None):\n","        targets = targets.view(-1, 1)\n","        pt = self.compute_probs(inputs)\n","        pt_adv = self.compute_probs(inputs_adv)\n","        if self.ensemble_training:\n","            pt_adv_second = self.compute_probs(second_inputs_adv)\n","            joint_pt = F.softmax((torch.log(pt) + torch.log(pt_adv) + torch.log(pt_adv_second)), dim=1)\n","        else:\n","            joint_pt = F.softmax((torch.log(pt) + self.poe_alpha*torch.log(pt_adv)), dim=1)\n","        joint_p = joint_pt.gather(1, targets)\n","        batch_loss = -torch.log(joint_p)\n","        if self.size_average:\n","            loss = batch_loss.mean()\n","        else:\n","            loss = batch_loss.sum()\n","        return loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RL0RU92KSmkI"},"outputs":[],"source":["# Multiplies the gradient of the given parameter by a constant.\n","class GradMulConst(torch.autograd.Function):\n","    @staticmethod\n","    def forward(ctx, x,  const):\n","       # The forward method computes the output of the function.\n","        # In this case, it simply returns the input tensor 'x' as is.\n","        # 'ctx' is a context object that can be used to stash information\n","        # for backward computation. Here, it stores the constant 'const'.\n","        ctx.const = const\n","        return x.view_as(x) # Returns the input tensor 'x' without altering it.\n","\n","    @staticmethod\n","    def backward(ctx, grad_output):\n","        # The backward method computes the gradient of the function with respect\n","        # to its input. 'grad_output' contains the gradient of the output with\n","        # respect to the input of this function.\n","        # The function multiplies 'grad_output' by the stored 'const'.\n","        return grad_output*ctx.const, None # Returns the modified gradient and None for 'const'.\n","\n","def grad_mul_const(x, const):\n","    # A convenience function for applying GradMulConst.\n","    # It applies the GradMulConst operation to a tensor 'x' with a multiplication constant 'const'.\n","    return GradMulConst.apply(x, const)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pJvIYYb8SmkI"},"outputs":[],"source":["\"\"\"\n","Main module for Debiasing.\n","\"\"\"\n","class ProductOfExperts(nn.Module):\n","    \"\"\"\n","    A module that wraps an NLI model and applies debiasing techniques.\n","    It supports various loss functions and ensemble methods for debiasing.\n","    \"\"\"\n","\n","    def __init__(self, dmodel, n_classes, NLInference_model):\n","        super(ProductOfExperts, self).__init__()\n","\n","        # Configuration and Initialization\n","        self.dmodel = dmodel\n","        self.n_classes = n_classes\n","        self.nli_model = NLInference_model        # The NLI model\n","\n","        # Initialize Product of Experts loss function\n","        self.poe_alpha = 1.0\n","        self.loss_fct = POELoss(poe_alpha=self.poe_alpha)\n","\n","        # Check if ensemble methods are used\n","        # Additional loss function for ensemble methods\n","        self.h_loss_weight = 1.0\n","        self.loss_fct_h = torch.nn.CrossEntropyLoss()\n","\n","        # Additional classifier for the hypothesis if ensemble methods are used - dimension of the classifier\n","        self.fc_dim = 512\n","        self.dclassifier = 2*self.dmodel\n","        self.c1 =  NonLinearClassifier(self.dmodel, self.fc_dim, self.n_classes)\n","\n","    def get_loss(self, output, adv_output, labels):\n","        \"\"\"\n","        Calculate the loss using the primary and adversarial outputs.\n","        It combines the primary loss and a weighted auxiliary loss.\n","        \"\"\"\n","        loss = self.loss_fct(output, labels, adv_output)\n","        h_output = adv_output\n","        loss += self.h_loss_weight * self.loss_fct_h(h_output, labels)\n","        return loss\n","\n","    def forward(self, s1, s2, labels):\n","        \"\"\"\n","        Forward pass for the model.\n","        It takes two sentences (or batches) and labels, computes NLI output,\n","        and calculates loss, including debiasing if enabled.\n","        \"\"\"\n","\n","        nli_output = self.nli_model(s1, s2)\n","        h_pred = None\n","\n","        # Embedding for the hypothesis with gradient blocking\n","        h_embeddings = self.nli_model.encoder(s2)\n","        # h_embeddings = torch.sum(h_embeddings,0) / h_embeddings.size()[0]\n","        h_embeddings = grad_mul_const(h_embeddings, 0.0) # do not backpropagate through the hypothesis encoder.\n","        h_pred = self.c1(h_embeddings)\n","\n","        # calculate loss\n","        loss1 = self.loss_fct(nli_output, labels, h_pred)\n","        h_output = h_pred\n","        loss2 = self.h_loss_weight * self.loss_fct_h(h_output, labels)\n","        total_loss = loss1 + loss2\n","\n","        # Collect outputs\n","        outputs = {}\n","        outputs['total_loss'] = total_loss\n","        outputs['nli'] = nli_output\n","        outputs['h'] = h_pred\n","        return outputs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Oc0U0_9FSmkI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1703290632805,"user_tz":0,"elapsed":503,"user":{"displayName":"Batu El","userId":"11666366648103508022"}},"outputId":"cfd76399-1db5-4265-8658-dce313e1b1d9"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n","  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"]}],"source":["dmodel = 256\n","n_classes = 2\n","NLI_model = NLInference(dmodel, n_classes)\n","POE_model = ProductOfExperts(dmodel, n_classes, NLI_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0bh5hMpqSmkI"},"outputs":[],"source":["\n","### TRAINING LOOP\n","import time\n","def train(dataloader):\n","    NLI_model.train()\n","    POE_model.train()\n","    NLI_model.cuda()\n","    POE_model.cuda()\n","\n","\n","    total_acc, total_count = 0, 0\n","    log_interval = 100\n","    start_time = time.time()\n","\n","    for idx, (s1, s2, label) in enumerate(dataloader):\n","        s1 = s1.to(DEVICE)\n","        s2 = s2.to(DEVICE)\n","        label = label.to(DEVICE)\n","\n","        optimizer.zero_grad()\n","\n","        output = POE_model(s1, s2, label)\n","        predicted_label = output['nli']\n","        loss = output['total_loss']\n","\n","        loss.backward()\n","        # torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1) # investigate\n","        optimizer.step()\n","\n","        total_acc += (predicted_label.argmax(1) == label).sum().item()\n","        total_count += label.size(0)\n","        train_acc = total_acc / total_count\n","\n","        if idx % log_interval == 0 and idx > 0:\n","            elapsed = time.time() - start_time\n","            print(\n","                \"| epoch {:3d} | {:5d}/{:5d} batches \"\n","                \"| accuracy {:8.3f}\".format(epoch, idx, len(dataloader), train_acc)\n","            )\n","            total_acc, total_count = 0, 0\n","            start_time = time.time()\n","\n","        train_losses.append(train_acc)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BGMDoyo_SmkI"},"outputs":[],"source":["### EVALUATION LOOP\n","def evaluate(dataloader):\n","    NLI_model.eval()\n","    POE_model.eval()\n","    # NLI_model.cuda()\n","    # POE_model.cuda()\n","\n","    total_acc, total_count = 0, 0\n","\n","    with torch.no_grad():\n","        for idx, (s1, s2, label) in enumerate(dataloader):\n","            s1 = s1.to(DEVICE)\n","            s2 = s2.to(DEVICE)\n","            label = label.to(DEVICE)\n","            output = POE_model(s1, s2, label)\n","            predicted_label = output['nli']\n","            # loss = output['total_loss']\n","            total_acc += (predicted_label.argmax(1) == label).sum().item()\n","            total_count += label.size(0)\n","    return total_acc / total_count"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yWrVWeNkSmkI"},"outputs":[],"source":["from torch.utils.data.dataset import random_split\n","from torchtext.data.functional import to_map_style_dataset\n","from torch.utils.data import DataLoader\n","\n","# Hyperparameters\n","EPOCHS = 20  # epoch\n","LEARNING_RATE = 0.0001  # learning rate\n","BATCH_SIZE = 16  # batch size for training\n","\n","criterion = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(POE_model.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.98), eps=1e-9)\n","# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n","\n","train_iter = nli_ds.NLIdataset(train_s1 , train_s2, train_label)\n","dev_matched_iter = nli_ds.NLIdataset(dev_matched_s1, dev_matched_s2 , dev_matched_label)\n","dev_mismatched_iter = nli_ds.NLIdataset(dev_mismatched_s1, dev_mismatched_s2 , dev_mismatched_label)\n","hans_iter = nli_ds.NLIdataset(hans_s1, hans_s2 , hans_label)\n","\n","train_dataset = to_map_style_dataset(train_iter)\n","dev_matched_dataset = to_map_style_dataset(dev_matched_iter)\n","dev_mismatched_dataset = to_map_style_dataset(dev_mismatched_iter)\n","hans_dataset = to_map_style_dataset(hans_iter)\n","\n","num_train = int(len(train_dataset) * 0.95)\n","split_train_, split_valid_ = random_split( train_dataset, [num_train, len(train_dataset) - num_train] )\n","num_train_hans = int(len(hans_dataset) * 0.75)\n","split_train_hans_ , split_test_hans_ = random_split( hans_dataset, [num_train_hans, len(hans_dataset) - num_train_hans] )\n","\n","def collate_fn( batch):\n","    label_pipeline = lambda x: int(x) #{'contradiction': 0, 'entailment': 1, 'neutral': 2, '-': -1}[x]\n","    # lists to hold processed source and target\n","    s1_batch, s2_batch, tgt_batch, padding_offsets = [], [], [],  []\n","    for s1_sample, s2_sample, tgt_sample in batch:\n","        # convert to tensor\n","        s1_sample = text_transform(s1_sample)\n","        s2_sample = text_transform(s2_sample)\n","        s1_batch.append(s1_sample)\n","        s2_batch.append(s2_sample)\n","        tgt_batch.append(label_pipeline(tgt_sample))\n","        padding_offsets.append(len(s1_sample))\n","        padding_offsets.append(len(s2_sample))\n","    # Convert the label_list to a tensor with integer type.\n","    tgt_batch = torch.tensor(tgt_batch, dtype=torch.int64)\n","    # to make the padded sequences for s1 and s2 equal length\n","    padding_offset = max(padding_offsets)\n","    s1_batch[0] = nn.ConstantPad1d((0,padding_offset - len(s1_batch[0]) ), tr.PAD_IDX)(s1_batch[0])\n","    s2_batch[0] = nn.ConstantPad1d((0,padding_offset - len(s2_batch[0]) ), tr.PAD_IDX)(s2_batch[0])\n","    # pad the sequences to ensure they have the same length\n","    s1_batch = torch.nn.utils.rnn.pad_sequence(s1_batch, padding_value=tr.PAD_IDX)\n","    s2_batch = torch.nn.utils.rnn.pad_sequence(s2_batch, padding_value=tr.PAD_IDX)\n","    return s1_batch.to(DEVICE), s2_batch.to(DEVICE), tgt_batch.to(DEVICE)\n","\n","train_dataloader = DataLoader( split_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn )\n","valid_dataloader = DataLoader( split_valid_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn )\n","dev_matched_dataloader = DataLoader( dev_matched_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn )\n","dev_mismatched_dataloader = DataLoader( dev_mismatched_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn )\n","\n","train_hans_dataloader = DataLoader( split_train_hans_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn )\n","test_hans_dataloader = DataLoader( split_test_hans_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mfxz5LUZSmkI","colab":{"base_uri":"https://localhost:8080/","height":453},"executionInfo":{"status":"error","timestamp":1703290644865,"user_tz":0,"elapsed":8892,"user":{"displayName":"Batu El","userId":"11666366648103508022"}},"outputId":"897b4459-e8fa-496b-9fe3-8b6b68ea73bd"},"outputs":[{"output_type":"stream","name":"stdout","text":["| epoch   1 |   100/15545 batches | accuracy    0.523\n","| epoch   1 |   200/15545 batches | accuracy    0.498\n","| epoch   1 |   300/15545 batches | accuracy    0.492\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-597dfbc0fd4b>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mepoch_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mepoch_end_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch_end_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mepoch_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-15-ab47ce4e7931>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataloader)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0ms1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0ms2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    675\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-17-0e38141abb36>\u001b[0m in \u001b[0;36mcollate_fn\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# convert to tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0ms1_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms1_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0ms2_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms2_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0ms1_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms1_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0ms2_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms2_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/../content/drive/MyDrive/Colab Notebooks/dis.experiments.4/utils/transforms.py\u001b[0m in \u001b[0;36mfunc\u001b[0;34m(txt_input)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mtxt_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtxt_input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchtext/data/utils.py\u001b[0m in \u001b[0;36m_basic_english_normalize\u001b[0;34m(line)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mpattern_re\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplaced_str\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_patterns_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpattern_re\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplaced_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["### INITIALIZATION\n","for p in POE_model.parameters():\n","    if p.dim() > 1:\n","        torch.nn.init.xavier_uniform_(p)\n","\n","### ACCOUNTING\n","model_paths = []\n","val_losses = []\n","train_losses = []\n","import time\n","model_id = '-'.join(time.ctime(time.time()).replace(':', ' ').split(' ')[2:5])\n","\n","### TRAINING\n","for epoch in range(1, EPOCHS + 1):\n","\n","    epoch_start_time = time.time()\n","    train(train_dataloader)\n","    epoch_end_time = time.time()\n","    elapsed_time = epoch_end_time - epoch_start_time\n","\n","    accu_train = train_losses[-1]\n","    accu_val = evaluate(valid_dataloader)\n","    val_losses.append(accu_val)\n","\n","    accu_dev_matched = evaluate(dev_matched_dataloader)\n","    accu_dev_mismatched = evaluate(dev_mismatched_dataloader)\n","    accu_hans = evaluate(test_hans_dataloader)\n","\n","    # register model path\n","    model_paths.append(f'id-{model_id}-epoch-{epoch}-accu_train-{accu_train:.3f}-accu_val-{accu_val:.3f}-accu_dev_matched-{accu_dev_matched:.3f}-accu_dev_mismatched-{accu_dev_mismatched:.3f}-accu_hans-{accu_hans:.3f}.pt')\n","    # save model to path\n","    torch.save(POE_model.state_dict(), drive_PATH+'/model_states/'+model_paths[-1])\n","\n","\n","    print(\"-\" * 59)\n","    print(\"| end of epoch {:3d} | time: {:5.2f}s | valid accuracy {:8.3f} |\".format( epoch, elapsed_time, accu_val))\n","    print(\"| dev matched accuracy {:8.3f} | dev mismatched accuracy {:8.3f} | hans accuracy {:8.3f} |\".format( accu_dev_matched, accu_dev_mismatched, accu_hans))\n","    print(\"-\" * 59)\n"]},{"cell_type":"code","source":["# Load the model with best validation accuracy\n","# best_model_index = np.argmax(val_losses)\n","# model_state = model_paths[best_model_index]\n","model_state = 'id-22-21-49-epoch-6-accu_train-0.710-accu_val-0.655-accu_dev_matched-0.661-accu_dev_mismatched-0.674-accu_hans-0.490.pt'\n","POE_model.load_state_dict(torch.load(drive_PATH+'/model_states/'+model_state))\n","# val_losses"],"metadata":{"id":"wjMbKpB6Ud7q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1703290652789,"user_tz":0,"elapsed":4016,"user":{"displayName":"Batu El","userId":"11666366648103508022"}},"outputId":"48b8a97b-cba4-4432-b95b-38e37d56bb6b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["# model_paths[best_model_index]"],"metadata":{"id":"C-Wi5EGuyTi3"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q-wtMuWrSmkI"},"outputs":[],"source":["import collections\n","from functools import partial\n","\n","NUM_BATCHES = 100\n","s1_dataset, s2_dataset, labels_dataset = [] , [], []\n","for idx, (s1, s2, label) in enumerate(valid_dataloader):\n","\tif idx == NUM_BATCHES:\n","\t\tbreak\n","\ts1_dataset.append(s1)\n","\ts2_dataset.append(s2)\n","\tlabels_dataset.append(label)\n","\n","# a dictionary that keeps saving the activations as they come\n","activations = collections.defaultdict(list)\n","def save_activation(name, mod, inp, out):\n","\tactivations[name].append(out.cpu())\n","\n","# Registering hooks for all the TransformerEncoder layers\n","# Note: Hooks are called EVERY TIME the module performs a forward pass. For modules that are\n","# called repeatedly at different stages of the forward pass (like TransformerEncoder in NLI called for s1 and s2 separately)\n","# this will save different activations.\n","# Editing the forward pass code to save activations is the way to go for these cases.\n","# Or we can filter out the odd and even indices from the activations to get the ones for s1 and s2\n","for name, m in POE_model.named_modules():\n","\tif name == 'nli_model.encoder':\n","\t\t# partial to assign the layer name to each hook\n","\t\tm.register_forward_hook(partial(save_activation, name))\n","\tif name == 'nli_model.classifier.classifier.1':\n","\t\tm.register_forward_hook(partial(save_activation, name))\n","\tif name == 'nli_model.classifier.classifier.4':\n","\t\tm.register_forward_hook(partial(save_activation, name))\n","\tif name == 'nli_model.classifier.classifier.7':\n","\t\tm.register_forward_hook(partial(save_activation, name))\n","\n","# forward pass through the full dataset\n","for batch_i in range(NUM_BATCHES):\n","\tout = POE_model(s1_dataset[batch_i], s2_dataset[batch_i], labels_dataset[batch_i])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bfZj8MsuSmkI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1703290702616,"user_tz":0,"elapsed":1310,"user":{"displayName":"Batu El","userId":"11666366648103508022"}},"outputId":"5577c3a3-f295-4edd-f492-63167d5afed6"},"outputs":[{"output_type":"stream","name":"stdout","text":["(256, 4800)\n","(512, 1600)\n","(512, 1600)\n","(2, 1600)\n"]}],"source":["keys = list(activations.keys())\n","activations_dict = {}\n","activations_df_dict = {}\n","for key in keys:\n","    activations_dict[key] = np.array(torch.cat([a.detach() for a in activations[key]])).transpose(1,0)\n","    activations_df_dict[key] = pd.DataFrame(activations_dict[key])\n","    print(activations_dict[key].shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yBXF5P4MSmkJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1703290714168,"user_tz":0,"elapsed":371,"user":{"displayName":"Batu El","userId":"11666366648103508022"}},"outputId":"a8697a0c-094c-403a-f70f-0d8e6227c4ea"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ True,  True,  True, ..., False, False, False])"]},"metadata":{},"execution_count":23}],"source":["# Corrections in the stored values is needed due to the repeated call of encoder\n","\n","idx0 = np.zeros(BATCH_SIZE)\n","idx1 = np.ones(BATCH_SIZE)\n","idx = []\n","for i in range(3*NUM_BATCHES):\n","    if (i % 3) == 0:\n","        idx.append(idx1)\n","    if (i % 3) == 1:\n","        idx.append(idx1)\n","    if (i % 3) == 2:\n","        idx.append(idx0)\n","idx = np.array(idx).flatten()\n","idx = idx != 0\n","idx"]},{"cell_type":"code","source":["activations_df_dict['nli_model.encoder'] = pd.DataFrame(activations_dict['nli_model.encoder'][:, idx])"],"metadata":{"id":"C3hZHl1PAa8Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# activations_dict['nli_model.encoder'] = activations_dict['nli_model.encoder'][:, idx]"],"metadata":{"id":"V5ya8b1AAYoJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VftUydC6SmkJ"},"outputs":[],"source":["activations_df = pd.concat(activations_df_dict, axis=0)\n","out_csv_PATH = drive_PATH + \"/res/activations/POE/test.csv\"\n","activations_df.to_csv(out_csv_PATH)"]},{"cell_type":"code","source":[],"metadata":{"id":"JaBbNVU-6JyG"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}